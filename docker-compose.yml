version: '3.8'

services:
  ml-server:
    build:
      context: .
      dockerfile: docker/ml_server/Dockerfile
    ports:
      - "8001:8001"
    environment:
      - ML_SERVER_PORT=8001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - energy-network

  api:
    build:
      context: .
      dockerfile: docker/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - API_PORT=8000
      - ML_SERVER_URL=http://ml-server:8001
      - SLM_MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
    depends_on:
      ml-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - energy-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  energy-network:
    driver: bridge
